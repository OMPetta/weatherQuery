{"paragraphs":[{"text":"//In Oshkosh, which is more common: days where the temperature was really cold (-10 or lower) at any point during the day or days where the temperature was hot (95 or higher) at any point during the day?\n\nimport org.apache.spark.sql._\n\nval df: DataFrame = spark.read.option(\"header\", true).csv(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\n\n// create view from dataframe to run sql against\ndf.createOrReplaceTempView(\"dfView\")\n\n// cast temp as double\nval oshkosh = spark.sql(\"SELECT CAST(TemperatureF as DOUBLE) as temp FROM dfView\")\n\n// filter temp remove -9999\nval oshkosh2  = oshkosh.filter($\"temp\" =!= -9999).select($\"temp\")\n\n// create some groups: less than negative ten and greater than 95\nval a = oshkosh2.filter($\"temp\" <= -10).select($\"temp\")\nval b = oshkosh2.filter($\"temp\" >= 95).select($\"temp\")\n\n// create views to run sql against\na.createOrReplaceTempView(\"aView\")\nb.createOrReplaceTempView(\"bView\")\n\n// get tip percentage per group\nval a_count = spark.sql(\"SELECT (count(temp)) as ColdCount FROM aView\")\nval b_count = spark.sql(\"SELECT (count(temp)) as HotCount FROM bView\")\n\n// print percentages\nprint(\"Count of Cold and Hot categories\")\na_count.show()\nb_count.show()","user":"anonymous","dateUpdated":"2020-05-02T22:11:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\ndf: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 14 more fields]\noshkosh: org.apache.spark.sql.DataFrame = [temp: double]\noshkosh2: org.apache.spark.sql.DataFrame = [temp: double]\na: org.apache.spark.sql.DataFrame = [temp: double]\nb: org.apache.spark.sql.DataFrame = [temp: double]\na_count: org.apache.spark.sql.DataFrame = [ColdCount: bigint]\nb_count: org.apache.spark.sql.DataFrame = [HotCount: bigint]\nCount of Cold and Hot categories+---------+\n|ColdCount|\n+---------+\n|      196|\n+---------+\n\n+--------+\n|HotCount|\n+--------+\n|      47|\n+--------+\n\n"}]},"apps":[],"jobName":"paragraph_1588362134130_-1702553171","id":"20200501-194214_1617948956","dateCreated":"2020-05-01T19:42:14+0000","dateStarted":"2020-05-02T22:10:12+0000","dateFinished":"2020-05-02T22:10:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1232"},{"text":"\n\n//Compute the average temperature (sum all temperatures in the time period and divide by the number of readings) for each season for Oshkosh and Iowa City. What is the difference in average temperatures for each season for Oshkosh vs Iowa City?\n\nimport org.apache.spark.sql._\nimport spark.implicits._ \n\nval df: DataFrame = spark.read.option(\"header\", true).csv(\"/user/maria_dev/final/Oshkosh/\")\nval df2: DataFrame = spark.read.option(\"header\", true).csv(\"/user/maria_dev/final/IowaCity/\")\n\n// create view from dataframe to run sql against\ndf.createOrReplaceTempView(\"dfView\")\n// create view from dataframe to run sql against\ndf2.createOrReplaceTempView(\"df2View\")\n\n// cast temp as double\nval oshkosh = spark.sql(\"SELECT CAST(Month as INT) as month, CAST(TemperatureF as DOUBLE) as temp FROM dfView\")\n// cast temp as double\nval iowa = spark.sql(\"SELECT CAST(Month as INT) as month, CAST(TemperatureF as DOUBLE) as temp FROM df2View\")\n\n// filter temp remove -9999\nval oshkosh2  = oshkosh.filter($\"temp\" =!= -9999).select($\"month\",$\"temp\")\n// filter temp remove -9999\nval iowa2  = iowa.filter($\"temp\" =!= -9999).select($\"month\",$\"temp\")\n\n// create some groups by seasons\nval okWinter = oshkosh2.filter($\"month\" === 12 || $\"month\" === 1 || $\"month\" === 2).select($\"month\",$\"temp\")\nval okSpring = oshkosh2.filter($\"month\" === 3 || $\"month\" === 4 || $\"month\" === 5).select($\"month\",$\"temp\")\nval okSummer = oshkosh2.filter($\"month\" === 6 || $\"month\" === 7 || $\"month\" === 8).select($\"month\",$\"temp\")\nval okFall = oshkosh2.filter($\"month\" === 9 || $\"month\" === 10 || $\"month\" === 11).select($\"month\",$\"temp\")\n// create some groups by seasons\nval ioWinter = iowa2.filter($\"month\" === 12 || $\"month\" === 1 || $\"month\" === 2).select($\"month\",$\"temp\")\nval ioSpring = iowa2.filter($\"month\" === 3 || $\"month\" === 4 || $\"month\" === 5).select($\"month\",$\"temp\")\nval ioSummer = iowa2.filter($\"month\" === 6 || $\"month\" === 7 || $\"month\" === 8).select($\"month\",$\"temp\")\nval ioFall = iowa2.filter($\"month\" === 9 || $\"month\" === 10 || $\"month\" === 11).select($\"month\",$\"temp\")\n\n// create view from dataframe to run sql against\nokWinter.createOrReplaceTempView(\"okWinterView\")\nokSpring.createOrReplaceTempView(\"okSpringView\")\nokSummer.createOrReplaceTempView(\"okSummerView\")\nokFall.createOrReplaceTempView(\"okFallView\")\n// create view from dataframe to run sql against\nioWinter.createOrReplaceTempView(\"ioWinterView\")\nioSpring.createOrReplaceTempView(\"ioSpringView\")\nioSummer.createOrReplaceTempView(\"ioSummerView\")\nioFall.createOrReplaceTempView(\"ioFallView\")\n\n// get average temp per group\nval okWinter2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgOkWinter FROM okWinterView\")\nval okSpring2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgOkSpring FROM okSpringView\")\nval okSummer2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgOkSummer FROM okSummerView\")\nval okFall2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgOkFall FROM okFallView\")\n// get average temp per group\nval ioWinter2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgIoWinter FROM ioWinterView\")\nval ioSpring2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgIoSpring FROM ioSpringView\")\nval ioSummer2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgIoSummer FROM ioSummerView\")\nval ioFall2 = spark.sql(\"SELECT (sum(temp)/count(temp)) as avgIoFall FROM ioFallView\")\n\n// add ids for join\nval okWinter3 = okWinter2.withColumn(\"id\", monotonically_increasing_id())\nval okSpring3 = okSpring2.withColumn(\"id\", monotonically_increasing_id())\nval okSummer3 = okSummer2.withColumn(\"id\", monotonically_increasing_id())\nval okFall3 = okFall2.withColumn(\"id\", monotonically_increasing_id())\n// add ids for join\nval ioWinter3 = ioWinter2.withColumn(\"id\", monotonically_increasing_id())\nval ioSpring3 = ioSpring2.withColumn(\"id\", monotonically_increasing_id())\nval ioSummer3 = ioSummer2.withColumn(\"id\", monotonically_increasing_id())\nval ioFall3 = ioFall2.withColumn(\"id\", monotonically_increasing_id())\n\n// join each season\nval winter = okWinter3.as(\"d1\").join(ioWinter3.as(\"d2\"), ($\"d1.id\" === $\"d2.id\"))\nval spring = okSpring3.as(\"d1\").join(ioSpring3.as(\"d2\"), ($\"d1.id\" === $\"d2.id\"))\nval summer = okSummer3.as(\"d1\").join(ioSummer3.as(\"d2\"), ($\"d1.id\" === $\"d2.id\"))\nval fall = okFall3.as(\"d1\").join(ioFall3.as(\"d2\"), ($\"d1.id\" === $\"d2.id\"))\n\n// create view from dataframe to run sql against\nwinter.createOrReplaceTempView(\"winterView\")\nspring.createOrReplaceTempView(\"springView\")\nsummer.createOrReplaceTempView(\"summerView\")\nfall.createOrReplaceTempView(\"fallView\")\n\n// get average temp difference per season\nval finalWinter = spark.sql(\"SELECT avgOkWinter as oshkosh, avgIoWinter as iowacity, (avgOkWinter-avgIoWinter) as winterdifference FROM winterView\")\nval finalSpring = spark.sql(\"SELECT avgOkSpring as oshkosh, avgIoSpring as iowacity, (avgOkSpring-avgIoSpring) as springDifference FROM springView\")\nval finalSummer = spark.sql(\"SELECT avgOkSummer as oshkosh, avgIoSummer as iowacity, (avgOkSummer-avgIoSummer) as summerDifference FROM summerView\")\nval finalFall = spark.sql(\"SELECT avgOkFall as oshkosh, avgIoFall as iowacity, (avgOkFall-avgIoFall) as fallDifference FROM fallView\")\n\nfinalWinter.show()\nfinalSpring.show()\nfinalSummer.show()\nfinalFall.show()\n\n\n","user":"anonymous","dateUpdated":"2020-05-03T02:03:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\nimport spark.implicits._\ndf: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 14 more fields]\ndf2: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 14 more fields]\noshkosh: org.apache.spark.sql.DataFrame = [month: int, temp: double]\niowa: org.apache.spark.sql.DataFrame = [month: int, temp: double]\noshkosh2: org.apache.spark.sql.DataFrame = [month: int, temp: double]\niowa2: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nokWinter: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nokSpring: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nokSummer: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nokFall: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nioWinter: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nioSpring: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nioSummer: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nioFall: org.apache.spark.sql.DataFrame = [month: int, temp: double]\nokWinter2: org.apache.spark.sql.DataFrame = [avgOkWinter: double]\nokSpring2: org.apache.spark.sql.DataFrame = [avgOkSpring: double]\nokSummer2: org.apache.spark.sql.DataFrame = [avgOkSummer: double]\nokFall2: org.apache.spark.sql.DataFrame = [avgOkFall: double]\nioWinter2: org.apache.spark.sql.DataFrame = [avgIoWinter: double]\nioSpring2: org.apache.spark.sql.DataFrame = [avgIoSpring: double]\nioSummer2: org.apache.spark.sql.DataFrame = [avgIoSummer: double]\nioFall2: org.apache.spark.sql.DataFrame = [avgIoFall: double]\nokWinter3: org.apache.spark.sql.DataFrame = [avgOkWinter: double, id: bigint]\nokSpring3: org.apache.spark.sql.DataFrame = [avgOkSpring: double, id: bigint]\nokSummer3: org.apache.spark.sql.DataFrame = [avgOkSummer: double, id: bigint]\nokFall3: org.apache.spark.sql.DataFrame = [avgOkFall: double, id: bigint]\nioWinter3: org.apache.spark.sql.DataFrame = [avgIoWinter: double, id: bigint]\nioSpring3: org.apache.spark.sql.DataFrame = [avgIoSpring: double, id: bigint]\nioSummer3: org.apache.spark.sql.DataFrame = [avgIoSummer: double, id: bigint]\nioFall3: org.apache.spark.sql.DataFrame = [avgIoFall: double, id: bigint]\nwinter: org.apache.spark.sql.DataFrame = [avgOkWinter: double, id: bigint ... 2 more fields]\nspring: org.apache.spark.sql.DataFrame = [avgOkSpring: double, id: bigint ... 2 more fields]\nsummer: org.apache.spark.sql.DataFrame = [avgOkSummer: double, id: bigint ... 2 more fields]\nfall: org.apache.spark.sql.DataFrame = [avgOkFall: double, id: bigint ... 2 more fields]\nfinalWinter: org.apache.spark.sql.DataFrame = [oshkosh: double, iowacity: double ... 1 more field]\nfinalSpring: org.apache.spark.sql.DataFrame = [oshkosh: double, iowacity: double ... 1 more field]\nfinalSummer: org.apache.spark.sql.DataFrame = [oshkosh: double, iowacity: double ... 1 more field]\nfinalFall: org.apache.spark.sql.DataFrame = [oshkosh: double, iowacity: double ... 1 more field]\n+-----------------+-----------------+-------------------+\n|          oshkosh|         iowacity|   winterdifference|\n+-----------------+-----------------+-------------------+\n|22.86142493745641|25.04653471771574|-2.1851097802593316|\n+-----------------+-----------------+-------------------+\n\n+-----------------+-----------------+------------------+\n|          oshkosh|         iowacity|  springDifference|\n+-----------------+-----------------+------------------+\n|44.76670739709886|50.20830046861242|-5.441593071513559|\n+-----------------+-----------------+------------------+\n\n+----------------+-----------------+-------------------+\n|         oshkosh|         iowacity|   summerDifference|\n+----------------+-----------------+-------------------+\n|69.1584675299853|72.55176916393103|-3.3933016339457254|\n+----------------+-----------------+-------------------+\n\n+-----------------+-----------------+-------------------+\n|          oshkosh|         iowacity|     fallDifference|\n+-----------------+-----------------+-------------------+\n|49.41173731871389|52.20284364734914|-2.7911063286352515|\n+-----------------+-----------------+-------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1588367068897_328278682","id":"20200501-210428_649185728","dateCreated":"2020-05-01T21:04:28+0000","dateStarted":"2020-05-03T02:03:04+0000","dateFinished":"2020-05-03T02:04:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1233"},{"text":"\n// For Oshkosh, what 7 day period was the hottest? \n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\n\n\nval df: DataFrame = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"/user/maria_dev/final/Oshkosh/\")\n    .withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n\n// filter temp remove -9999\nval df2  = df.filter($\"TemperatureF\" =!= -9999).select($\"fullDay\",$\"TemperatureF\",$\"Month\",$\"Day\",$\"Year\")\n\n// create view from dataframe to run sql against\ndf2.createOrReplaceTempView(\"dfView\")\n\n// cast temp as double\nval df3 = spark.sql(\"SELECT CAST(fullDay as INT) as fullDay, TemperatureF, Month, Day, Year FROM dfView\")\n\nval windowSpec = Window.orderBy(\"fullDay\").rangeBetween(-3,3)\n\nval answer =df3.withColumn(\"weeklyAverage\", avg(df3(\"TemperatureF\")).over(windowSpec))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n\n// create view to run sql against\nanswer.createOrReplaceTempView(\"answerView\")\n\n// get average per group\nval a = spark.sql(\"SELECT DISTINCT(fullDay) as middleWeek, weeklyAverage AS averageTemp, month, day, year FROM answerView ORDER BY averageTemp DESC LIMIT 10 \")\nprint(\"The days listed here are the middle of the week with the highest average temperature\")\na.show()\n\n\n","user":"anonymous","dateUpdated":"2020-05-03T03:34:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\ndf: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 15 more fields]\ndf2: org.apache.spark.sql.DataFrame = [fullDay: string, TemperatureF: double ... 3 more fields]\ndf3: org.apache.spark.sql.DataFrame = [fullDay: int, TemperatureF: double ... 3 more fields]\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1142d1e6\nanswer: org.apache.spark.sql.DataFrame = [fullDay: int, TemperatureF: double ... 4 more fields]\na: org.apache.spark.sql.DataFrame = [middleWeek: int, averageTemp: double ... 3 more fields]\nThe days listed here are the middle of the week with the highest average temperature+----------+-----------------+-----+---+----+\n|middleWeek|      averageTemp|month|day|year|\n+----------+-----------------+-----+---+----+\n|    201273|82.15126582278482|    7|  3|2012|\n|    201272|81.15632911392407|    7|  2|2012|\n|    200187|81.08355263157895|    8|  7|2001|\n|    201274| 80.8888888888889|    7|  4|2012|\n|    201275|80.60641711229948|    7|  5|2012|\n|   2006731|80.43883495145631|    7| 31|2006|\n|    201276|80.22222222222224|    7|  6|2012|\n|    201277|80.03636363636363|    7|  7|2012|\n|    200186|79.96521739130434|    8|  6|2001|\n|   2013716|79.61714285714288|    7| 16|2013|\n+----------+-----------------+-----+---+----+\n\n"}]},"apps":[],"jobName":"paragraph_1588367069100_447166092","id":"20200501-210429_548110818","dateCreated":"2020-05-01T21:04:29+0000","dateStarted":"2020-05-03T03:34:39+0000","dateFinished":"2020-05-03T03:34:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1234"},{"text":"\n\n// Once you have determined the coldest hour for each day, return the hour that has the highest frequency. \n\nimport org.apache.spark.sql._\n// import spark.implicits._ \n// read data\nval df: DataFrame = spark.read.option(\"header\", true).csv(\"/user/maria_dev/final/Oshkosh/\")\n\n// split TimeCST to get fullTime and  ampm\nval df2 = df.withColumn(\"tempTimeOne\", split($\"TimeCST\", \" \"))\n  .withColumn(\"fullTime\", $\"tempTimeOne\"(0))\n  .withColumn(\"tempAmpm\", $\"tempTimeOne\"(1))\n  .withColumn(\"ampm\", ltrim(col(\"tempAmpm\")))\n  .drop(\"tempTimeOne\")\n  \n// split fullTime to get hour and minute\nval df3 = df2.withColumn(\"tempTimeTwo\", split($\"fullTime\", \":\"))\n  .withColumn(\"hour\", $\"tempTimeTwo\"(0))\n  .withColumn(\"minute\", $\"tempTimeTwo\"(1))\n  .drop(\"tempTimeTwo\")\n\n// create view from dataframe to run sql against\ndf3.createOrReplaceTempView(\"df3View\")\n\n// cast types\nval df4 = spark.sql(\"SELECT CAST(Year as INT) as year, CAST(Month as INT) as month, CAST(Day as INT) as day, CAST(hour as INT) as hour, CAST(minute as INT) as minute, CAST(TemperatureF as DOUBLE) as temp, ampm as ampm FROM df3View\")\n\n// filter\nval df5  = df4.filter($\"temp\" =!= -9999).select($\"year\",$\"month\",$\"day\",$\"hour\",$\"minute\",$\"temp\",$\"ampm\")\n\nval df6 = df5.withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n    .withColumn(\"fullHour\", concat($\"hour\", $\"ampm\"))\n    .withColumn(\"fullDayHour\", concat($\"Year\", $\"Month\", $\"Day\", $\"hour\", $\"ampm\"))\n\nval df7 = df6.groupBy(\"fullDay\", \"hour\", \"ampm\").avg( \"temp\")\n\nval df8 = df7.groupBy(\"fullDay\").min(\"avg(temp)\")\n\nval df9 = df6.as(\"d6\").join(df8.as(\"d8\"), ($\"d6.fullDay\" === $\"d8.fullDay\")).select($\"d6.*\", $\"d8.*\")\n\nval df11 = df9.filter($\"temp\" === $\"min(avg(temp))\")\n\n// create view from dataframe to run sql against\ndf11.createOrReplaceTempView(\"df11View\")\n\n// get count for hour freq\nval df12 = spark.sql(\"SELECT first(fullHour), count(fullHour) AS hourFreq FROM df11View GROUP BY fullHour ORDER BY hourFreq DESC \")\n// print results\ndf12.show()\n","user":"anonymous","dateUpdated":"2020-05-03T19:12:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\ndf: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 14 more fields]\ndf2: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 17 more fields]\ndf3: org.apache.spark.sql.DataFrame = [Year: string, Month: string ... 19 more fields]\ndf4: org.apache.spark.sql.DataFrame = [year: int, month: int ... 5 more fields]\ndf5: org.apache.spark.sql.DataFrame = [year: int, month: int ... 5 more fields]\ndf6: org.apache.spark.sql.DataFrame = [year: int, month: int ... 8 more fields]\ndf7: org.apache.spark.sql.DataFrame = [fullDay: string, hour: int ... 2 more fields]\ndf8: org.apache.spark.sql.DataFrame = [fullDay: string, min(avg(temp)): double]\ndf9: org.apache.spark.sql.DataFrame = [year: int, month: int ... 10 more fields]\ndf11: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [year: int, month: int ... 10 more fields]\ndf12: org.apache.spark.sql.DataFrame = [first(fullHour, false): string, hourFreq: bigint]\n+----------------------+--------+\n|first(fullHour, false)|hourFreq|\n+----------------------+--------+\n|                   5AM|    1343|\n|                  11PM|    1218|\n|                   4AM|    1038|\n|                   6AM|     944|\n|                   3AM|     711|\n|                   2AM|     523|\n|                  12AM|     518|\n|                   1AM|     509|\n|                  10PM|     451|\n|                   7AM|     368|\n|                   9PM|     215|\n|                   8AM|     107|\n|                   8PM|     106|\n|                   7PM|      74|\n|                   6PM|      54|\n|                   9AM|      53|\n|                   5PM|      41|\n|                   4PM|      36|\n|                  10AM|      29|\n|                   3PM|      21|\n+----------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1588367069295_1654123392","id":"20200501-210429_769235179","dateCreated":"2020-05-01T21:04:29+0000","dateStarted":"2020-05-03T19:07:16+0000","dateFinished":"2020-05-03T19:07:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1235"},{"text":"\n\n// Which city had a time period of 24 hours or less that saw the largest temperature difference? \n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\n\n// read Oshkosh data\nval df: DataFrame = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"/user/maria_dev/final/Oshkosh/\")\n    .withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n    .withColumn(\"fullDayTime\", concat($\"Year\", lit(\"-\"), $\"Month\", lit(\"-\"), $\"Day\", lit(\" \"), $\"TimeCST\"))\n    .withColumn(\"timeStamp\", unix_timestamp(concat($\"fullDayTime\"), \"yyyy-M-d h:m a\"))\n\n// split TimeCST to get fullTime and  ampm\nval df2 = df.withColumn(\"tempTimeOne\", split($\"TimeCST\", \" \"))\n  .withColumn(\"fullTime\", $\"tempTimeOne\"(0))\n  .withColumn(\"tempAmpm\", $\"tempTimeOne\"(1))\n  .withColumn(\"ampm\", ltrim(col(\"tempAmpm\")))\n  .drop(\"tempTimeOne\")\n  \n// split fullTime to get hour and minute\nval df3 = df2.withColumn(\"tempTimeTwo\", split($\"fullTime\", \":\"))\n  .withColumn(\"hour\", $\"tempTimeTwo\"(0))\n  .withColumn(\"minute\", $\"tempTimeTwo\"(1))\n  .drop(\"tempTimeTwo\")\n\n// filter temp remove -9999\nval df4  = df3.filter($\"TemperatureF\" =!= -9999).select($\"TemperatureF\", $\"timeStamp\", $\"fullTime\", $\"hour\", $\"minute\", $\"fullDay\",$\"Month\",$\"Day\",$\"Year\",$\"fullDayTime\")\n\nval windowSpec = Window.orderBy(\"timeStamp\").rangeBetween(-43200,43200)\n\nval minOk =df4.withColumn(\"weeklyMin\", min(df3(\"TemperatureF\")).over(windowSpec))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \nval maxOk =df4.withColumn(\"weeklyMax\", max(df3(\"TemperatureF\")).over(windowSpec))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \nval df5 = minOk.as(\"d1\").join(maxOk.as(\"d2\"), ($\"d1.timeStamp\" === $\"d2.timeStamp\")).select($\"d1.*\", $\"d2.weeklyMax\")\n\nval df6 = df5.withColumn(\"diffMaxMin\", ($\"weeklyMax\"-$\"weeklyMin\"))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \n// create view from dataframe to run sql against\ndf6.createOrReplaceTempView(\"df6View\")\n\n// get count for hour freq\nval df7 = spark.sql(\"SELECT fullDayTime, diffMaxMin FROM (SELECT fullDayTime, diffMaxMin, DENSE_RANK() OVER (ORDER BY diffMaxMin DESC) AS ranked FROM df6View) subquery WHERE subquery.ranked = 1 \")\n\n//\n// REPEAT FOR IOWACITY\n//\n\n// read data IowaCity\nval df8: DataFrame = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"/user/maria_dev/final/IowaCity/\")\n    .withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n    .withColumn(\"fullDayTime\", concat($\"Year\", lit(\"-\"), $\"Month\", lit(\"-\"), $\"Day\", lit(\" \"), $\"TimeCST\"))\n    .withColumn(\"timeStamp\", unix_timestamp(concat($\"fullDayTime\"), \"yyyy-M-d h:m a\"))\n\n// split TimeCST to get fullTime and  ampm\nval df9 = df8.withColumn(\"tempTimeOne\", split($\"TimeCST\", \" \"))\n  .withColumn(\"fullTime\", $\"tempTimeOne\"(0))\n  .withColumn(\"tempAmpm\", $\"tempTimeOne\"(1))\n  .withColumn(\"ampm\", ltrim(col(\"tempAmpm\")))\n  .drop(\"tempTimeOne\")\n  \n// split fullTime to get hour and minute\nval df11 = df9.withColumn(\"tempTimeTwo\", split($\"fullTime\", \":\"))\n  .withColumn(\"hour\", $\"tempTimeTwo\"(0))\n  .withColumn(\"minute\", $\"tempTimeTwo\"(1))\n  .drop(\"tempTimeTwo\")\n\n// filter temp remove -9999\nval df12  = df11.filter($\"TemperatureF\" =!= -9999).select($\"TemperatureF\", $\"timeStamp\", $\"fullTime\", $\"hour\", $\"minute\", $\"fullDay\",$\"Month\",$\"Day\",$\"Year\",$\"fullDayTime\")\n\nval windowSpec = Window.orderBy(\"timeStamp\").rangeBetween(-43200,43200)\n\nval minIo =df12.withColumn(\"weeklyMin\", min(df11(\"TemperatureF\")).over(windowSpec))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \nval maxIo =df12.withColumn(\"weeklyMax\", max(df11(\"TemperatureF\")).over(windowSpec))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \nval df13 = minIo.as(\"d3\").join(maxIo.as(\"d4\"), ($\"d3.timeStamp\" === $\"d4.timeStamp\")).select($\"d3.*\", $\"d4.weeklyMax\")\n\nval df14 = df13.withColumn(\"diffMaxMin\", ($\"weeklyMax\"-$\"weeklyMin\"))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"day\", $\"Day\")\n    .withColumn(\"year\", $\"Year\")\n    .withColumn(\"fullTime\", $\"fullTime\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDay\", $\"fullDay\")\n    .withColumn(\"timeStamp\", $\"timeStamp\")\n    .withColumn(\"minute\", $\"minute\")\n    .withColumn(\"fullDayTime\", $\"fullDayTime\")\n    \n// create view from dataframe to run sql against\ndf14.createOrReplaceTempView(\"df14View\")\n\n// get count for hour freq\nval df15 = spark.sql(\"SELECT fullDayTime, diffMaxMin FROM (SELECT fullDayTime, diffMaxMin, DENSE_RANK() OVER (ORDER BY diffMaxMin DESC) AS ranked FROM df14View) subquery WHERE subquery.ranked = 1 \")\n\n// print results\nprint(\"The table below shows the middle of all 24 hour periods which had the highest difference in temperature and that difference for the Iowa City\")\ndf15.show()\nprint(\"The table below shows the middle of all 24 hour periods which had the highest difference in temperature and that difference for the Oshkosh City\")\ndf7.show()    \n","user":"anonymous","dateUpdated":"2020-05-03T20:59:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\ndf: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\ndf2: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 20 more fields]\ndf3: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\ndf4: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 8 more fields]\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6e0e5717\nminOk: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 9 more fields]\nmaxOk: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 9 more fields]\ndf5: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 10 more fields]\ndf6: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 11 more fields]\ndf7: org.apache.spark.sql.DataFrame = [fullDayTime: string, diffMaxMin: double]\ndf8: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\ndf9: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 20 more fields]\ndf11: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\ndf12: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 8 more fields]\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@226ccf07\nminIo: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 9 more fields]\nmaxIo: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 9 more fields]\ndf13: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 10 more fields]\ndf14: org.apache.spark.sql.DataFrame = [TemperatureF: double, timeStamp: bigint ... 11 more fields]\ndf15: org.apache.spark.sql.DataFrame = [fullDayTime: string, diffMaxMin: double]\nThe table below shows the middle of all 24 hour periods which had the highest difference in temperature and that difference for the Iowa City+-------------------+----------+\n|        fullDayTime|diffMaxMin|\n+-------------------+----------+\n| 2008-12-14 6:52 PM|      56.2|\n| 2008-12-14 7:39 PM|      56.2|\n| 2008-12-14 7:52 PM|      56.2|\n| 2008-12-14 8:05 PM|      56.2|\n| 2008-12-14 8:52 PM|      56.2|\n| 2008-12-14 9:30 PM|      56.2|\n| 2008-12-14 9:52 PM|      56.2|\n|2008-12-14 10:04 PM|      56.2|\n|2008-12-14 10:08 PM|      56.2|\n|2008-12-14 10:16 PM|      56.2|\n|2008-12-14 10:23 PM|      56.2|\n|2008-12-14 10:34 PM|      56.2|\n|2008-12-14 10:52 PM|      56.2|\n|2008-12-14 11:03 PM|      56.2|\n|2008-12-14 11:28 PM|      56.2|\n|2008-12-14 11:44 PM|      56.2|\n|2008-12-14 11:52 PM|      56.2|\n|2008-12-14 11:59 PM|      56.2|\n|2008-12-15 12:16 AM|      56.2|\n+-------------------+----------+\n\nThe table below shows the middle of all 24 hour periods which had the highest difference in temperature and that difference for the Oshkosh City+-----------------+----------+\n|      fullDayTime|diffMaxMin|\n+-----------------+----------+\n|2008-1-29 5:53 PM|      51.1|\n|2008-1-29 6:02 PM|      51.1|\n|2008-1-29 6:12 PM|      51.1|\n|2008-1-29 6:20 PM|      51.1|\n|2008-1-29 6:53 PM|      51.1|\n|2008-1-29 7:08 PM|      51.1|\n|2008-1-29 7:53 PM|      51.1|\n|2008-1-29 8:10 PM|      51.1|\n+-----------------+----------+\n\n"}]},"apps":[],"jobName":"paragraph_1588367071081_1162414298","id":"20200501-210431_430030498","dateCreated":"2020-05-01T21:04:31+0000","dateStarted":"2020-05-03T20:41:01+0000","dateFinished":"2020-05-03T20:41:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1236"},{"text":"\n\n//  For each month, provide the hour (e.g. 7am, 5pm, etc) and city that is the best time to run.  averaging all temperatures with the same city and same hour and checking how far that average is from 50 degrees. If there is a tie, a tiebreaker will be the least windy hour (i.e. the windspeed column) on average. If there is still a tie, both hours and cities are reported.\n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\n\n// read data\nval df: DataFrame = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"/user/maria_dev/final/Oshkosh/\")\n    .withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n    .withColumn(\"fullDayTime\", concat($\"Year\", lit(\"-\"), $\"Month\", lit(\"-\"), $\"Day\", lit(\" \"), $\"TimeCST\"))\n    .withColumn(\"timeStamp\", unix_timestamp(concat($\"fullDayTime\"), \"yyyy-M-d h:m a\"))\n\n// split TimeCST to get fullTime and  ampm\nval df2 = df.withColumn(\"tempTimeOne\", split($\"TimeCST\", \" \"))\n  .withColumn(\"fullTime\", $\"tempTimeOne\"(0))\n  .withColumn(\"tempAmpm\", $\"tempTimeOne\"(1))\n  .withColumn(\"ampm\", ltrim(col(\"tempAmpm\")))\n  .drop(\"tempTimeOne\")\n  \n// split fullTime to get hour and minute\nval df3 = df2.withColumn(\"tempTimeTwo\", split($\"fullTime\", \":\"))\n  .withColumn(\"hour\", $\"tempTimeTwo\"(0))\n  .withColumn(\"minute\", $\"tempTimeTwo\"(1))\n  .drop(\"tempTimeTwo\")\n  \n// create view from dataframe to run sql against\ndf3.createOrReplaceTempView(\"df3View\")\n\n// cast types\nval df4 = spark.sql(\"SELECT CAST(timeStamp as INT) as timeStamp, CAST(Year as INT) as year, CAST(Month as INT) as month, CAST(Day as INT) as day, CAST(hour as INT) as hour, CAST(minute as INT) as minute, CAST(TemperatureF as DOUBLE) as temp, ampm as ampm, fullDayTime as fullDayTime, fullDay as fullDay, fullTime as fullTime  FROM df3View\")\n  \n// filter temp remove -9999\nval df5  = df4.filter($\"temp\" =!= -9999).select($\"temp\", $\"timeStamp\", $\"fullTime\", $\"hour\", $\"minute\", $\"fullDay\",$\"Month\",$\"Day\",$\"Year\",$\"fullDayTime\", $\"ampm\")\n// get average temp by month per hour\nval df6 = df5.groupBy(\"month\", \"hour\", \"ampm\").avg( \"temp\")\n\n// get differance from 50\nval df7 = df6.withColumn(\"diff50\", ($\"avg(temp)\"-50))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"ampm\", $\"ampm\")\n\n// get absolute value of that difference\nval df8 = df7.withColumn(\"diff50Abs\", abs($\"diff50\"))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"ampm\", $\"ampm\")\n    \n// create view from dataframe to run sql against\ndf8.createOrReplaceTempView(\"df8View\")\n\n// get count for hour freq\nval df9 = spark.sql(\"SELECT month, hour, ampm, diff50Abs FROM (SELECT month, hour, ampm, diff50Abs, DENSE_RANK() OVER (ORDER BY diff50Abs ASC) AS ranked FROM df8View) subquery WHERE subquery.ranked = 5 \")\n\n//\n// repeat for iowacity\n//\n\n// read iowa  data\nval df11: DataFrame = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"/user/maria_dev/final/IowaCity/\")\n    .withColumn(\"fullDay\", concat($\"Year\", $\"Month\", $\"Day\"))\n    .withColumn(\"fullDayTime\", concat($\"Year\", lit(\"-\"), $\"Month\", lit(\"-\"), $\"Day\", lit(\" \"), $\"TimeCST\"))\n    .withColumn(\"timeStamp\", unix_timestamp(concat($\"fullDayTime\"), \"yyyy-M-d h:m a\"))\n\n// split TimeCST to get fullTime and  ampm\nval df12 = df11.withColumn(\"tempTimeOne\", split($\"TimeCST\", \" \"))\n  .withColumn(\"fullTime\", $\"tempTimeOne\"(0))\n  .withColumn(\"tempAmpm\", $\"tempTimeOne\"(1))\n  .withColumn(\"ampm\", ltrim(col(\"tempAmpm\")))\n  .drop(\"tempTimeOne\")\n  \n// split fullTime to get hour and minute\nval df13 = df12.withColumn(\"tempTimeTwo\", split($\"fullTime\", \":\"))\n  .withColumn(\"hour\", $\"tempTimeTwo\"(0))\n  .withColumn(\"minute\", $\"tempTimeTwo\"(1))\n  .drop(\"tempTimeTwo\")\n  \n// create view from dataframe to run sql against\ndf13.createOrReplaceTempView(\"df13View\")\n\n// cast types\nval df14 = spark.sql(\"SELECT CAST(timeStamp as INT) as timeStamp, CAST(Year as INT) as year, CAST(Month as INT) as month, CAST(Day as INT) as day, CAST(hour as INT) as hour, CAST(minute as INT) as minute, CAST(TemperatureF as DOUBLE) as temp, ampm as ampm, fullDayTime as fullDayTime, fullDay as fullDay, fullTime as fullTime  FROM df13View\")\n  \n// filter temp remove -9999\nval df15  = df14.filter($\"temp\" =!= -9999).select($\"temp\", $\"timeStamp\", $\"fullTime\", $\"hour\", $\"minute\", $\"fullDay\",$\"Month\",$\"Day\",$\"Year\",$\"fullDayTime\", $\"ampm\")\n// get average temp by month per hour\nval df16 = df15.groupBy(\"month\", \"hour\", \"ampm\").avg( \"temp\")\n\n// get differance from 50\nval df17 = df16.withColumn(\"diff50\", ($\"avg(temp)\"-50))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"ampm\", $\"ampm\")\n\n// get absolute value of that difference\nval df18 = df17.withColumn(\"diff50Abs\", abs($\"diff50\"))\n    .withColumn(\"month\", $\"Month\")\n    .withColumn(\"hour\", $\"hour\")\n    .withColumn(\"ampm\", $\"ampm\")\n    \n// create view from dataframe to run sql against\ndf18.createOrReplaceTempView(\"df18View\")\n\n// get count for hour freq\nval df19 = spark.sql(\"SELECT month, hour, ampm, diff50Abs FROM (SELECT month, hour, ampm, diff50Abs, DENSE_RANK() OVER (ORDER BY diff50Abs ASC) AS ranked FROM df18View) subquery WHERE subquery.ranked = 5 \")\n\n// print results\nprint(\"The following table shows the month with the hour which has the average temperature closest to 50 for Iowa City\")\ndf19.show()\nprint(\"The following table shows the month with the hour which has the average temperature closest to 50 for Oshkosh City\")\ndf9.show()\n","user":"anonymous","dateUpdated":"2020-05-03T22:40:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql._\nimport org.apache.spark.sql.expressions._\ndf: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\ndf2: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 20 more fields]\ndf3: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\ndf4: org.apache.spark.sql.DataFrame = [timeStamp: int, year: int ... 9 more fields]\ndf5: org.apache.spark.sql.DataFrame = [temp: double, timeStamp: int ... 9 more fields]\ndf6: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 2 more fields]\ndf7: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 3 more fields]\ndf8: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 4 more fields]\ndf9: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 2 more fields]\ndf11: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 17 more fields]\ndf12: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 20 more fields]\ndf13: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 22 more fields]\ndf14: org.apache.spark.sql.DataFrame = [timeStamp: int, year: int ... 9 more fields]\ndf15: org.apache.spark.sql.DataFrame = [temp: double, timeStamp: int ... 9 more fields]\ndf16: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 2 more fields]\ndf17: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 3 more fields]\ndf18: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 4 more fields]\ndf19: org.apache.spark.sql.DataFrame = [month: int, hour: int ... 2 more fields]\nThe following table shows the month with the hour which has the average temperature closest to 50 for Iowa City+-----+----+----+------------------+\n|month|hour|ampm|         diff50Abs|\n+-----+----+----+------------------+\n|   10|  11|  PM|0.7253234750462099|\n+-----+----+----+------------------+\n\nThe following table shows the month with the hour which has the average temperature closest to 50 for Oshkosh City+-----+----+----+-------------------+\n|month|hour|ampm|          diff50Abs|\n+-----+----+----+-------------------+\n|    4|  12|  PM|0.35999999999999943|\n+-----+----+----+-------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1588470302801_1501504649","id":"20200503-014502_1808590739","dateCreated":"2020-05-03T01:45:02+0000","dateStarted":"2020-05-03T22:40:07+0000","dateFinished":"2020-05-03T22:40:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1237"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1588530971143_-1876287197","id":"20200503-183611_1073420250","dateCreated":"2020-05-03T18:36:11+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1238"}],"name":"prob1AF","id":"2F8T5BDNE","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}